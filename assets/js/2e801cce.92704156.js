"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[450],{6029:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"song tracking","metadata":{"permalink":"/song tracking","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/spotify/index.md","source":"@site/blog/spotify/index.md","title":"How I Track My Spotify Songs","description":"Spotify Wrapped is cool, but I wanted to gain a more in-depth understanding of my music listening habits. Unfortunately, Spotify\'s API doesn\'t provide full access to your listening history. The next best option is to fetch the latest 50 tracks you\'ve listened to. With this limitation in mind, I set out to create a project that could start recording all the songs I listened to and perform data analysis on them.","date":"2022-01-26T00:00:00.000Z","formattedDate":"January 26, 2022","tags":[{"label":"cloud","permalink":"/tags/cloud"},{"label":"webdev","permalink":"/tags/webdev"},{"label":"spotify","permalink":"/tags/spotify"}],"readingTime":1.565,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"song tracking","title":"How I Track My Spotify Songs","tags":["cloud","webdev","spotify"],"date":"2022-01-26T00:00:00.000Z"},"nextItem":{"title":"IWild Cam 2019","permalink":"/iwild cam 2019"}},"content":"Spotify Wrapped is cool, but I wanted to gain a more in-depth understanding of my music listening habits. Unfortunately, Spotify\'s API doesn\'t provide full access to your listening history. The next best option is to fetch the latest 50 tracks you\'ve listened to. With this limitation in mind, I set out to create a project that could start recording all the songs I listened to and perform data analysis on them.\\n\\n\x3c!--truncate--\x3e\\n\\n## Architecture\\n\\nThe basic architecture of the app consists of a serverless function that runs every 6 hours to fetch the latest songs you\'ve listened to and stores them in a database. I can then query this database to perform various analyses. Additionally, I developed a web app that acts as an API, allowing users to register to have their own history tracked and proivide a full history of their songs.\\n\\nTo implement this I decided to leverage AWS services for fetching and storing the latest songs, while deploying the web app for registration and song viewing on Heroku.\\n\\nBelow is a high-level architecture diagram illustrating the components involved:\\n\\n![Architecture Diagram](./spotify-hist-arch.png)\\n\\n## Registration/Viewing\\n\\nFor registration and song viewing, I utilized [Spotify](https://github.com/spotipy-dev/spotipy) for interfacing with Spotify\'s API and built the web app using Flask. The app is then deployed on Heroku, providing a seamless experience for users to register and access their tracked songs.\\n\\n## Saving History\\n\\nTo save the song history, I employed CloudWatch to trigger a function every 6 hours. This function fetches the users from the database and retrieves their latest songs, which are then added to DynamoDB for storage.\\n\\n![Sequence Diagram](./fetch-songs-seq.png)\\n\\nBy tracking all the songs I listen to over the next year, I\'ll be able to create a more comprehensive Spotify Wrapped experience, delving deeper into my music preferences and habits.\\n\\nHere is a [LINK](https://github.com/ShaunSpinelli/spotify_history) to the repo and the specfic [lambda](https://github.com/ShaunSpinelli/spotify_history/blob/master/lambdas/lambda_function.py) for fetcing the latest songs."},{"id":"iwild cam 2019","metadata":{"permalink":"/iwild cam 2019","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/i-wild-cam-2019/index.md","source":"@site/blog/i-wild-cam-2019/index.md","title":"IWild Cam 2019","description":"Competition Page","date":"2020-01-22T00:00:00.000Z","formattedDate":"January 22, 2020","tags":[{"label":"kaggle","permalink":"/tags/kaggle"}],"readingTime":3.26,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"iwild cam 2019","title":"IWild Cam 2019","tags":["kaggle"],"date":"2020-01-22T00:00:00.000Z"},"prevItem":{"title":"How I Track My Spotify Songs","permalink":"/song tracking"},"nextItem":{"title":"UDA - Unsupervised Data Augmentation","permalink":"/unsupervised data"}},"content":"[Competition Page](https://www.kaggle.com/c/iwildcam-2019-fgvc6/overview)\\n\\n> \\"how do you classify a species in a new region that you may not have seen in previous training data?\\". \\n\\nIn essence this challenge asks you to identify different species of animals from trail cam images.\\n\\n\x3c!--truncate--\x3e\\n\\nI used fastai as it is an awesome library that allows for fast prototyping with built in best practices and feature like learning rate scheduling. Their data api also saves a lot of time in preprocessing data and creating train, validation and test sets. This allows you more time to focus on improving the performance of your solution. Also fastai V3 Part2 is coming out next month so I wanted to get comfortable with the library before tackling that course.\\n\\n![trail_cam_images](./trail_cam_images.png)\\n\\nAs you can see from the above images there is a range of challenges in this data set. Including small regions of interest (top left), images taken at night(top middle) and perspective (top right) to name just a few.\\n\\nAnother issue was the huge class imbalances present in the data set.  In the graph below you can see the class distributions. The empty class (which had 131,457 samples, over 67% of the training data) was removed. The mountain lion (22) had the fewest samples with only 33 training examples.\\n\\n![samples](./n_samples_chart.png)\\n\\nMy initial thought was I needed to train on large images if my model had any chance of detecting the smaller animals. So probably starting off with a smaller network like resnet34 the that would allow me to train on bigger crops sizes while still maintaining a reasonable batch size.\\n\\nThe first approach was to remove all the empty class images, train a model on only images containing animal then use the class probabilities and some threshold. eg: If the model didn\'t predict a class with a confidence of at least 0.4 the i would set the class to empty. This approach didn\'t work well with  a public LB score of 0.108. It was hard to find a threshold that worked well. Although I could have tried some other approaches to find a good threshold(run inference on the entire train set and test various how thresholds performed) I decided to try a second approach.\\n\\nMy second approach involved creating binary classifier, which would just predict if an animal was in an image. I then combined the results from these two models. If the animal detector model detects an animal I use the images class prediction as its final prediction other wise it will just be predicted as empty. This got me a public LB score of 0.156 and shot me up to 17th  place.\\n\\nLooking at the class probabilities I could see that there was still opportunity for improvement.\\n\\n![confidences](./confidences.png)\\n\\nSome predictions from the animal detector where right on the threshold, predicting no animal was in image with 0.5001 confidence while the animal classifier was predicting a elk with a 0.999 confidence(see above second last row). After finding a few more of these examples and checking the images I knew that there was potential to improve mmy results. I could try another threshold approach but thought a small Neural Net might be a fun solution.\\n\\nI used a small two layer neural network. Although my training and validation results were good the results didn\'t carry through to the test set. So I knew the issue was probably in the accuracy of my classifier and animal detector.\\n\\nUnfortunately I ran out of time to carry on working on this challenge but there are a couple of things i could have tried to improve accuracy.\\n\\n1) Utilising saliency maps to crop out the regions of interest so the model as more of the animal in the image.\\n\\n2) More data augmentation that replicates common features in the images eg: occlusion, darkness \\n\\n> Note: There were bounding box annotations for the a subset of the training data but i choose not to use them"},{"id":"unsupervised data","metadata":{"permalink":"/unsupervised data","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/UDA/index.md","source":"@site/blog/UDA/index.md","title":"UDA - Unsupervised Data Augmentation","description":"I look into a paper that introduces a fresh perspective on the role of advanced data augmentation methods in semi-supervised learning, showcasing remarkable results across various language and vision tasks. I show the key findings and dive into code examples that highlight the power of these techniques","date":"2019-08-20T00:00:00.000Z","formattedDate":"August 20, 2019","tags":[{"label":"papers","permalink":"/tags/papers"},{"label":"deeplearning","permalink":"/tags/deeplearning"}],"readingTime":4.405,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"unsupervised data","title":"UDA - Unsupervised Data Augmentation","tags":["papers","deeplearning"],"date":"2019-08-20T00:00:00.000Z"},"prevItem":{"title":"IWild Cam 2019","permalink":"/iwild cam 2019"}},"content":"I look into a paper that introduces a fresh perspective on the role of advanced data augmentation methods in semi-supervised learning, showcasing remarkable results across various language and vision tasks. I show the key findings and dive into code examples that highlight the power of these techniques\\n\\n[[Paper]](https://arxiv.org/abs/1904.12848) [[Blog Post]](https://ai.googleblog.com/2019/07/advancing-semi-supervised-learning-with.html)\\n\\n\x3c!--truncate--\x3e\\n\\nAll code snippets taken from this [repository](https://github.com/google-research/uda)\\n\\n### Key Ideas and Findings\\n\\n- Improving classification models with unlabeled data, data augmentation and semi-supervised learning. \\n\\n- Combining both labelled and unlabeled and using a custom loss function you can train a classifier for both computer vision and nlp problems.\\n\\n- Matches and outperforms supervised learning approaches with only a fraction of the data. \\n\\n\\n### Key Results\\n\\n1 - Using CIFAR-10 with 4k examples, UDA achieves an error rate of 5.27, matching the performance of a fully supervised model that uses 50k examples\\n\\n2 - UDA achieves a new state-of-the-art error rate with a more than 45% reduction in error rate compared to the previous best semi-supervised result.\\n\\n3 - On SVHN, UDA achieves an error rate of 2.46 with only 1k labeled examples, matching the performance of the fully supervised model trained with ~70k labeled examples.\\n\\n\\n## Components\\n\\n###  Loss Function\\n\\nThis loss function is calculated by combining two loss functions one for the supervised part of the data (labelled) and one for the unsupervised part of the data (UDA). There is also a weighting factor **\u03bb** applied to the UDA loss function, researchers set **\u03bb** to 1 for most of their experiments.\\n \\n**Supervised Loss (Labelled Data)-**  Cross entropy\\n\\n**Unsupervised Consistency Loss (Unlabelled Data)-** more specifically KL divergence, it is calculated between predicted distributions on the unlabeled examples and unlabeled augmented example.\\n\\n### Implementation of KL-divergence\\n\\n```python title=\\"uda/image/main.py\\"\\n# q_logits are the augmented logits \\ndef _kl_divergence_with_logits(p_logits, q_logits):\\n    p = tf.nn.softmax(p_logits)\\n    log_p = tf.nn.log_softmax(p_logits)\\n    log_q = tf.nn.log_softmax(q_logits)\\n    kl = tf.reduce_sum(p * (log_p - log_q), -1)\\nreturn kl\\n\\n```\\n\\n![loss function](./4-Figure1-1.png)\\n\\n## Augmentation Strategies\\n\\nImage augmentation was done using  **AutoAugment** [[Paper]](https://ai.googleblog.com/2018/06/improving-deep-learning-performance.html) [[Medium]](https://towardsdatascience.com/how-to-improve-your-image-classifier-with-googles-autoaugment-77643f0be0c9) and **Cutout** [[Paper]](https://arxiv.org/abs/1708.04552) was also used for experiments with CIFAR-10 and SVHN. It is noted that there is a trade off between having diverse augmented training examples and at the same time keeping the ground truth label matched. Many augmented samples where created for one ground truth and were all generated offline before training.\\n\\nText augmentation was done using back translation and a process called TF-IDF.\\n\\n![UDA Data augmentation](./UDA-augmentation.png)\\n\\n**Results from ablation study on different augmentation strategies**\\n\\n![augmentation results](./aug-results.png)\\n\\n\\n## Additional Training Techniques\\n\\n### Training Signal Annealing\\n\\nBy training on sch a small number of training samples there was a risk of the model overfitting to the labelled data. To negate that researches used (TSA) with the idea being to *gradually release training signals of the labeled data without overfitting as the model is trained on more and more unlabeled examples.*\\n\\nWhen the probability of the correct category is higher the some threshold then that example is removed from the loss calculation. This threshold prevents the model from over-training on examples it is already confident on.\\n\\nThree threshold schedules were suggested with the decision on which to use dependant on the different ratios of labeled and unlabeled data.\\n\\n**log-schedule** where threshold is increased most rapidly at the beginning of training. Used when the model is **less** likely to overfit because of abundance of labeled examples or effective regularizations in the model.\\n\\n**exp-schedule** where threshold is increased most rapidly at the end of training. Used when the problem is **more** likely to overfit because of a relatively easy problem or the number of training examples is limited. The supervising signal get mostly released by the end of training.\\n\\n**linear-schedule** where the threshold is linearly increased while training.\\n\\n![training signal schedules](./5-Figure2-1.png)\\n\\n**Results from ablation study on different TSA schedules**\\n\\n![training signal results](./tsa-results.png)\\n\\n### Implementation of TSA\\n\\n```python title=\\"uda/image/main.py\\"\\n\\ndef get_tsa_threshold(schedule, global_step, num_train_steps, start, end):\\n  step_ratio = tf.to_float(global_step) / tf.to_float(num_train_steps)\\n  if schedule == \\"linear_schedule\\":\\n    coeff = step_ratio\\n  elif schedule == \\"exp_schedule\\":\\n    scale = 5\\n    # [exp(-5), exp(0)] = [1e-2, 1]\\n    coeff = tf.exp((step_ratio - 1) * scale)\\n  elif schedule == \\"log_schedule\\":\\n    scale = 5\\n    # [1 - exp(0), 1 - exp(-5)] = [0, 0.99]\\n    coeff = 1 - tf.exp((-step_ratio) * scale)\\n  return coeff * (end - start) + start\\n\\n# Usage \\n# line 258\\n eff_train_prob_threshold = get_tsa_threshold(\\n      FLAGS.tsa, global_step, FLAGS.train_steps,\\n      tsa_start, end=1)\\n\\n# line 276\\n larger_than_threshold = tf.greater(\\n      correct_label_probs, eff_train_prob_threshold)\\n\\n```\\n\\n### Sharpening Predictions\\n\\n- **Confidence-based masking** - If the predictions on the labelled data is below some confidence threshold then these predictions are removed from the final loss calculation, leaving only predictions with the highest confidence probabilities.\\n\\n### Implementation\\n```python title=\\"uda/image/main.py\\"\\n\\nori_prob = tf.nn.softmax(ori_logits, axis=-1)\\nlargest_prob = tf.reduce_max(ori_prob, axis=-1)\\nloss_mask = tf.cast(tf.greater(largest_prob, FLAGS.uda_confidence_thresh), tf.float32)\\n# metric_dict[\\"unsup/high_prob_ratio\\"] = tf.reduce_mean(loss_mask)\\nloss_mask = tf.stop_gradient(loss_mask)\\naug_loss = aug_loss * loss_mask\\n# metric_dict[\\"unsup/high_prob_loss\\"] = tf.reduce_mean(aug_loss)\\n\\n```\\n\\n- **Entropy minimization**\\n\\n- **Softmax temperature controlling**\\n\\n### Domain-relevance Data filtering\\n\\nIf the class distribution between the labeled and unlabeled data sets are unmatched it can hurt the performance of the model. To try and match the class distributions between the labeled and unlabeled data a model was trained on the labeled samples and used to infer the unlabeled samples class. Examples that the model was most confident on where then used for training.\\n\\n### Comparison to current approaches\\n\\n![compare other apporaches](./compare-results.png)"}]}')}}]);